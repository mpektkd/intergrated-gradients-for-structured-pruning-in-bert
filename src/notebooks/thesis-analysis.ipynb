{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVnjgohNKFtG"
      },
      "source": [
        "# Correlation between Attention and Attribution by IG on BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0n93ChiJKTSe",
        "outputId": "d5970653-2091-4720-d521-f4dae906e9ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Collecting captum\n",
            "  Downloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from captum) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from captum) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from captum) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from captum) (4.66.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->captum) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->captum) (1.3.0)\n",
            "Installing collected packages: captum\n",
            "Successfully installed captum-0.7.0\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: pyarrow-hotfix, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install captum\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mask Language Modeling"
      ],
      "metadata": {
        "id": "F5UbOLptfMYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### lib"
      ],
      "metadata": {
        "id": "MVYfvZ3p0kXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import scipy\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import random\n",
        "from torch.utils.data import Sampler\n",
        "\n",
        "import torch\n",
        "import random\n",
        "\n",
        "from captum.attr import LayerConductance\n",
        "from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import PreTrainedTokenizer, DataCollatorForLanguageModeling\n",
        "\n",
        "def predict(model, inputs, token_type_ids=None, position_ids=None, attention_mask=None):\n",
        "\n",
        "    output = model(inputs, token_type_ids=token_type_ids,\n",
        "                 position_ids=position_ids, attention_mask=attention_mask, )\n",
        "\n",
        "    return output.attentions\n",
        "\n",
        "def squad_pos_forward_func(inputs, model, attention_mask=None, target=None):\n",
        "    '''\n",
        "      Notes:\n",
        "        1. The need of target is necessary when the output is multi-dimensional.\n",
        "    '''\n",
        "    output = model(inputs_embeds=inputs, attention_mask=attention_mask )\n",
        "\n",
        "    return output.logits.max(1).values if target==None else output.logits\n",
        "\n",
        "def normalize_columns(dataset: Dataset, old_name: str, new_name: str):\n",
        "    \"\"\"\n",
        "    Renames a column in the dataset and drop all but 'label' and 'text'.\n",
        "\n",
        "    Args:\n",
        "    dataset (Dataset): The HuggingFace dataset to process.\n",
        "    old_name (str): The current name of the column to be renamed.\n",
        "    new_name (str): The new name for the column.\n",
        "\n",
        "    Returns:\n",
        "    Dataset: The dataset with the renamed column.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if the old column name exists in the dataset\n",
        "    if old_name in dataset.column_names:\n",
        "        # Rename the column\n",
        "        dataset = dataset.rename_column(old_name, new_name)\n",
        "    else:\n",
        "        print(f\"The column '{old_name}' does not exist in the dataset.\")\n",
        "\n",
        "    # List all columns except 'label'\n",
        "    to_be_removed = [col for col in dataset.column_names if col != 'label' and col != 'text']\n",
        "\n",
        "    dataset = dataset.remove_columns(to_be_removed)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def tokenize_and_filter(dataset: Dataset, tokenizer: PreTrainedTokenizer, max_token_length: int = 200, batch_size: int = 1000):\n",
        "    \"\"\"\n",
        "    Tokenizes the sentences in the dataset and filters out examples with more than max_token_length tokens.\n",
        "\n",
        "    Args:\n",
        "    dataset (Dataset): The HuggingFace dataset to process.\n",
        "    tokenizer (PreTrainedTokenizer): The tokenizer to use for tokenizing the sentences.\n",
        "    max_token_length (int): Maximum allowed token length for each example.\n",
        "\n",
        "    Returns:\n",
        "    Dataset: A new dataset with tokenized sentences and filtered based on token length.\n",
        "    \"\"\"\n",
        "\n",
        "    def batch_tokenize(examples):\n",
        "\n",
        "      tokens = [tokenizer.tokenize(text) for text in examples['text']]\n",
        "\n",
        "      return {'tokens': tokens, 'label': examples['label']}\n",
        "\n",
        "    # Tokenize the dataset\n",
        "    tokenized_dataset = dataset.map(\n",
        "        batch_tokenize,\n",
        "        batched=True,\n",
        "        batch_size=batch_size,\n",
        "        remove_columns=['text']\n",
        "    )\n",
        "\n",
        "    # Filter the dataset to keep examples with <= max_token_length tokens\n",
        "    filtered_dataset = tokenized_dataset.filter(\n",
        "        lambda example: len(example['tokens']) <= max_token_length\n",
        "    )\n",
        "\n",
        "    return filtered_dataset\n",
        "\n",
        "\n",
        "class MovieReviewDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, reviews, targets, tokenizer):\n",
        "    self.reviews = reviews\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "\n",
        "  # The sentence is alreadt tokenized\n",
        "  def __getitem__(self, item):\n",
        "    tokens = self.reviews[item]\n",
        "    target = self.targets[item]\n",
        "\n",
        "    return tokens, target\n",
        "\n",
        "def create_data_loader(df, batch_size, tokenizer, collator, bucket_sampling=False, shuffle=False):\n",
        "\n",
        "  ds = MovieReviewDataset(\n",
        "    reviews=df.tokens.to_numpy(),\n",
        "    targets=df.label.to_numpy(),\n",
        "    tokenizer=tokenizer\n",
        "  )\n",
        "\n",
        "  # Implement Bucket Sampling\n",
        "  if bucket_sampling:\n",
        "    return torch.utils.data.DataLoader(\n",
        "        ds,\n",
        "        batch_sampler=BatchSamplerSimilarLength(dataset=ds, batch_size=batch_size, shuffle=shuffle),\n",
        "        collate_fn=collator\n",
        "    )\n",
        "\n",
        "  return torch.utils.data.DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=collator\n",
        "  )\n",
        "\n",
        "def get_predictions(model, device, data_loader):\n",
        "  '''\n",
        "  Notes for MLM model:\n",
        "    1. The MLM model finally produces a tensor (batch, tokens, vocab_size), that for every token\n",
        "    produces a probability for each token in the vocab. Based on each fine tuning task, we add the\n",
        "    appropriate head in the end and make the choise.\n",
        "  '''\n",
        "  total_attentions = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in tqdm(data_loader, total=len(data_loader), desc=\"Extracting Attention Weights\", ncols=75, leave=True, bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt}\"):\n",
        "\n",
        "      # we do not use 'label' as we do no care about the loss\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      print(input_ids.shape)\n",
        "\n",
        "      attentions = predict(\n",
        "        model,\n",
        "        inputs=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "\n",
        "      # stack all the layers -> layer x batch x head x tokens x tokens\n",
        "      attentions_stack = torch.stack(tuple(i.detach().clone().cpu() for i in attentions)) # transfer data from cuda to cpu, therefore attentions_stack is on CPU\n",
        "\n",
        "      del attentions\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      # stack all the attentions of the spescific batch -> layer x batch x head x tokens x tokens\n",
        "      total_attentions.append(attentions_stack.detach().clone().cpu())\n",
        "      # total_attentions = torch.cat((total_attentions, attentions_stack), dim=1)\n",
        "\n",
        "      del attentions_stack\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "  return total_attentions\n",
        "\n",
        "def get_interpretability_scores(model, device, data_loader, target=None):\n",
        "  '''\n",
        "    Notes:\n",
        "      1. In the source code for LayerConductance.attribute() all the helper-functions\n",
        "      that are called, activate the grad computation (requires_grad=True) before start\n",
        "      the calculation.\n",
        "      https://github.com/pytorch/captum/blob/ed3b1fa4b3d8afc0eff4179b1d1ef4b191f13cc1/captum/_utils/gradient.py#L589\n",
        "  '''\n",
        "  interpretable_embedding = configure_interpretable_embedding_layer(model, 'bert.embeddings.word_embeddings')\n",
        "  try:\n",
        "    total_attributions = []\n",
        "    for d in tqdm(data_loader, total=len(data_loader), desc=\"Calculating Attribution Scores\", ncols=75, leave=True, bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt}\"):\n",
        "\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      ref_input_ids = d[\"ref_input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "\n",
        "      print(input_ids.shape)\n",
        "\n",
        "      input_embeddings = interpretable_embedding.indices_to_embeddings(input_ids)\n",
        "      ref_input_embeddings = interpretable_embedding.indices_to_embeddings(ref_input_ids)\n",
        "\n",
        "      layer_attn_mat = []\n",
        "      for i in range(model.config.num_hidden_layers):\n",
        "        lc = LayerConductance(squad_pos_forward_func, model.bert.encoder.layer[i])\n",
        "        layer_attributions = lc.attribute(inputs=input_embeddings, baselines=ref_input_embeddings, target=target, additional_forward_args=(model, attention_mask, target))\n",
        "        layer_attn_mat.append(layer_attributions[1].detach().clone().cpu()) # convert into cpu memory and the delete from cuda to avoid memory leak\n",
        "\n",
        "        # Delete intermediate tensors to free up memory\n",
        "        del layer_attributions, lc\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "      # stack all the layers -> layer x batch x head x tokens x tokens\n",
        "      layer_attn_mat = torch.stack(layer_attn_mat)\n",
        "\n",
        "      del input_ids, ref_input_ids, attention_mask, input_embeddings, ref_input_embeddings\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      # # stack all the attentions of the specific batch -> layer x batch x head x tokens x tokens\n",
        "      total_attributions.append(layer_attn_mat.detach().clone().cpu())\n",
        "\n",
        "      del layer_attn_mat\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    # total_attributions = torch.cat(attributions_list, dim=1)\n",
        "\n",
        "  finally:\n",
        "    # after we finish the interpretation we need to remove\n",
        "    # interpretable embedding layer with the following command:\n",
        "    remove_interpretable_embedding_layer(model, interpretable_embedding)\n",
        "\n",
        "\n",
        "  return total_attributions\n",
        "\n",
        "def get_interpretability_scores2(model, device, data_loader, _target=None, mlm=False):\n",
        "  '''\n",
        "    Notes:\n",
        "      1. In the source code for LayerConductance.attribute() all the helper-functions\n",
        "      that are called, activate the grad computation (requires_grad=True) before start\n",
        "      the calculation.\n",
        "      https://github.com/pytorch/captum/blob/ed3b1fa4b3d8afc0eff4179b1d1ef4b191f13cc1/captum/_utils/gradient.py#L589\n",
        "  '''\n",
        "  interpretable_embedding = configure_interpretable_embedding_layer(model, 'bert.embeddings.word_embeddings')\n",
        "  try:\n",
        "    total_attributions = []\n",
        "    for d in tqdm(data_loader, total=len(data_loader), desc=\"Calculating Attribution Scores\", ncols=75, leave=True, bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt}\"):\n",
        "\n",
        "      _input_ids = d[\"input_ids\"]\n",
        "      _ref_input_ids = d[\"ref_input_ids\"]\n",
        "      _attention_mask = d[\"attention_mask\"]\n",
        "      print('_input_ids.shape', _input_ids.shape)\n",
        "\n",
        "      # BertForLanguafeModeling\n",
        "      _target = d[\"target\"] if mlm else _target\n",
        "\n",
        "      attribution_list_of_copies = []\n",
        "      for (input_ids, ref_input_ids, attention_mask, target) in zip(_input_ids, _ref_input_ids, _attention_mask, _target):\n",
        "        input_ids = input_ids.unsqueeze(0).to(device)\n",
        "        ref_input_ids = ref_input_ids.unsqueeze(0).to(device)\n",
        "        attention_mask = attention_mask.unsqueeze(0).to(device)\n",
        "\n",
        "        input_embeddings = interpretable_embedding.indices_to_embeddings(input_ids)\n",
        "        ref_input_embeddings = interpretable_embedding.indices_to_embeddings(ref_input_ids)\n",
        "\n",
        "        layer_attn_mat = []\n",
        "        for i in range(model.config.num_hidden_layers):\n",
        "          lc = LayerConductance(squad_pos_forward_func, model.bert.encoder.layer[i])\n",
        "          layer_attributions = lc.attribute(inputs=input_embeddings, baselines=ref_input_embeddings, target=target, additional_forward_args=(model, attention_mask, target))\n",
        "          layer_attn_mat.append(layer_attributions[1].detach().clone().cpu()) # convert into cpu memory and the delete from cuda to avoid memory leak\n",
        "\n",
        "          # Delete intermediate tensors to free up memory\n",
        "          del layer_attributions, lc\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "        # (12,1,12,tokens,tokens)\n",
        "        layer_attn_mat = torch.stack(layer_attn_mat)\n",
        "\n",
        "        # attribution_list_of_copies is a list of attribution for each copy of the sent\n",
        "        attribution_list_of_copies.append(layer_attn_mat)\n",
        "\n",
        "        del layer_attn_mat, input_ids, ref_input_ids, attention_mask, input_embeddings, ref_input_embeddings\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "      # stack the list to take the mean value\n",
        "      attribution_list_of_copies = torch.cat(attribution_list_of_copies, dim=1)\n",
        "      total_attributions.append(torch.mean(attribution_list_of_copies, dim=1))\n",
        "\n",
        "      del attribution_list_of_copies\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    # total_attributions = torch.cat(attributions_list, dim=1)\n",
        "\n",
        "  finally:\n",
        "    # after we finish the interpretation we need to remove\n",
        "    # interpretable embedding layer with the following command:\n",
        "    remove_interpretable_embedding_layer(model, interpretable_embedding)\n",
        "\n",
        "\n",
        "  return total_attributions\n",
        "\n",
        "def get_predictions2(model, device, data_loader):\n",
        "  '''\n",
        "  Notes for MLM model:\n",
        "    1. The MLM model finally produces a tensor (batch, tokens, vocab_size), that for every token\n",
        "    produces a probability for each token in the vocab. Based on each fine tuning task, we add the\n",
        "    appropriate head in the end and make the choise.\n",
        "  '''\n",
        "  total_attentions = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in tqdm(data_loader, total=len(data_loader), desc=\"Extracting Attention Weights\", ncols=75, leave=True, bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt}\"):\n",
        "\n",
        "      # we do not use 'label' as we do no care about the loss\n",
        "      _input_ids = d[\"input_ids\"]\n",
        "      _attention_mask = d[\"attention_mask\"]\n",
        "      # print('_input_ids.shape', _input_ids.shape)\n",
        "      attention_list_of_copies = []\n",
        "      for (input_ids, attention_mask) in zip(_input_ids, _attention_mask):\n",
        "\n",
        "        input_ids = input_ids.unsqueeze(0).to(device)\n",
        "        attention_mask = attention_mask.unsqueeze(0).to(device)\n",
        "        # print('input_ids.shape', input_ids.shape)\n",
        "        attentions = predict(\n",
        "          model,\n",
        "          inputs=input_ids,\n",
        "          attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        # stack all the layers -> layer x batch x head x tokens x tokens\n",
        "        attentions_stack = torch.stack(tuple(i.detach().clone().cpu() for i in attentions)) # transfer data from cuda to cpu, therefore attentions_stack is on CPU\n",
        "        # print('attentions_stack.shape', attentions_stack.shape)\n",
        "        attention_list_of_copies.append(attentions_stack)\n",
        "\n",
        "        del attentions, attentions_stack\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "      # stack the attention for all the copies and take the mean value\n",
        "      attention_list_of_copies = torch.cat(attention_list_of_copies, dim=1)\n",
        "      # print('attention_list_of_copies.shape', attention_list_of_copies.shape)\n",
        "      total_attentions.append(torch.mean(attention_list_of_copies, dim=1))\n",
        "      # total_attentions = torch.cat((total_attentions, attentions_stack), dim=1)\n",
        "\n",
        "      del attention_list_of_copies\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "  return total_attentions\n",
        "\n",
        "def get_matrices(model, device, data_loader):\n",
        "  '''\n",
        "    Notes:\n",
        "      1. In the source code for LayerConductance.attribute() all the helper-functions\n",
        "      that are called, activate the grad computation (requires_grad=True) before start\n",
        "      the calculation.\n",
        "      https://github.com/pytorch/captum/blob/ed3b1fa4b3d8afc0eff4179b1d1ef4b191f13cc1/captum/_utils/gradient.py#L589\n",
        "  '''\n",
        "  total_attentions, total_attributions = [], []\n",
        "  for d in tqdm(data_loader, total=len(data_loader), desc=\"Calculating Both Matrices. It may take a while :P...\", ncols=75, leave=True, bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt}\"):\n",
        "\n",
        "    _input_ids = d[\"input_ids\"]\n",
        "    _ref_input_ids = d[\"ref_input_ids\"]\n",
        "    _attention_mask = d[\"attention_mask\"]\n",
        "    _target = d[\"target\"]\n",
        "    print(_input_ids.shape)\n",
        "\n",
        "    attention_list_of_copies, attribution_list_of_copies = [], []\n",
        "    for (input_ids, ref_input_ids, attention_mask, target) in zip(_input_ids, _ref_input_ids, _attention_mask, _target):\n",
        "      input_ids = input_ids.unsqueeze(0).to(device)\n",
        "      ref_input_ids = ref_input_ids.unsqueeze(0).to(device)\n",
        "      attention_mask = attention_mask.unsqueeze(0).to(device)\n",
        "      # Attention Calculation\n",
        "      #############################################\n",
        "      attentions = predict(\n",
        "        model,\n",
        "        inputs=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "\n",
        "      # stack all the layers -> layer x batch x head x tokens x tokens\n",
        "      attentions_stack = torch.stack(tuple(i.detach().clone().cpu() for i in attentions)) # transfer data from cuda to cpu, therefore attentions_stack is on CPU\n",
        "      # print('attentions_stack.shape', attentions_stack.shape)\n",
        "      attention_list_of_copies.append(attentions_stack)\n",
        "\n",
        "      del attentions, attentions_stack\n",
        "      torch.cuda.empty_cache()\n",
        "      #############################################\n",
        "\n",
        "      # Attribution Calculation\n",
        "\n",
        "      #############################################\n",
        "      try:\n",
        "        interpretable_embedding = configure_interpretable_embedding_layer(model, 'bert.embeddings.word_embeddings')\n",
        "\n",
        "        input_embeddings = interpretable_embedding.indices_to_embeddings(input_ids)\n",
        "        ref_input_embeddings = interpretable_embedding.indices_to_embeddings(ref_input_ids)\n",
        "\n",
        "        layer_attn_mat = []\n",
        "        for i in range(model.config.num_hidden_layers):\n",
        "          lc = LayerConductance(squad_pos_forward_func, model.bert.encoder.layer[i])\n",
        "          layer_attributions = lc.attribute(inputs=input_embeddings, baselines=ref_input_embeddings, target=target, additional_forward_args=(model, attention_mask, target))\n",
        "          layer_attn_mat.append(layer_attributions[1].detach().clone().cpu()) # convert into cpu memory and the delete from cuda to avoid memory leak\n",
        "\n",
        "          # Delete intermediate tensors to free up memory\n",
        "          del layer_attributions, lc\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "        # (12,1,12,tokens,tokens)\n",
        "        layer_attn_mat = torch.stack(layer_attn_mat)\n",
        "\n",
        "        # attribution_list_of_copies is a list of attribution for each copy of the sent\n",
        "        attribution_list_of_copies.append(layer_attn_mat)\n",
        "\n",
        "        del layer_attn_mat, input_embeddings, ref_input_embeddings\n",
        "        torch.cuda.empty_cache()\n",
        "      finally:\n",
        "        # after we finish the interpretation we need to remove\n",
        "        # interpretable embedding layer with the following command:\n",
        "        remove_interpretable_embedding_layer(model, interpretable_embedding)\n",
        "\n",
        "      #############################################\n",
        "\n",
        "    # stack the attention for all the copies and take the mean value\n",
        "    # We use unsqueeze here, because we take mean value and the batch dimension is lost.\n",
        "    # However, we need this dimensio because the mean() is an aggregation fucn of the original\n",
        "    # sample, so batch = 1.\n",
        "    attention_list_of_copies = torch.cat(attention_list_of_copies, dim=1)\n",
        "    attention_mean_of_copies = torch.mean(attention_list_of_copies, dim=1).unsqueeze(1)\n",
        "    total_attentions.append(attention_mean_of_copies)\n",
        "\n",
        "    del attention_list_of_copies\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # stack the list to take the mean value\n",
        "    attribution_list_of_copies = torch.cat(attribution_list_of_copies, dim=1)\n",
        "    attribution_mean_of_copies = torch.mean(attribution_list_of_copies, dim=1).unsqueeze(1)\n",
        "    total_attributions.append(attribution_mean_of_copies)\n",
        "\n",
        "    del attribution_list_of_copies\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  return total_attentions, total_attributions\n",
        "\n",
        "def create_nested_directories(config):\n",
        "    # Start with the base directory\n",
        "    current_path = '../scores/' + config['dataset'] + '/'\n",
        "\n",
        "    # Check if the base directory exists, if not, create it\n",
        "    if not os.path.exists(current_path):\n",
        "        os.makedirs(current_path)\n",
        "\n",
        "    # Iterate over the remaining configuration parameters\n",
        "    for key in ['target', 'max_len', 'model_info']:\n",
        "        # Append the next level directory to the current path\n",
        "        current_path = os.path.join(current_path, config[key])\n",
        "\n",
        "        # Check if this subdirectory exists, if not, create it\n",
        "        if not os.path.exists(current_path):\n",
        "            os.makedirs(current_path)\n",
        "\n",
        "    current_path += '/'\n",
        "\n",
        "    return current_path\n",
        "\n",
        "def calculate_percentages(numbers, condition, params):\n",
        "    numbers = np.array(numbers)\n",
        "\n",
        "    percentages = {}\n",
        "    indices = {}\n",
        "    for p in params:\n",
        "        count = np.sum(condition(numbers, p))\n",
        "        ind = np.where(condition(numbers, p))\n",
        "\n",
        "        percentages[p] = (count / len(numbers)) * 100 if len(numbers) != 0 else 0\n",
        "        indices[p] = ind[0]\n",
        "\n",
        "    return percentages, indices\n",
        "\n",
        "\n",
        "def f(range):\n",
        "  bottom = range[0]\n",
        "  if (bottom == 0.2):\n",
        "    return 1\n",
        "  if (bottom == 0.4):\n",
        "    return 2\n",
        "  if (bottom == 0.6):\n",
        "    return 3\n",
        "  return 4\n",
        "\n",
        "class Collator(object):\n",
        "  def __init__(self, tokenizer, **kwargs):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.kwargs = kwargs\n",
        "\n",
        "  def __call__(self, batch):\n",
        "\n",
        "    tokens, label = [], []\n",
        "    for _tokens, _label in batch:\n",
        "      tokens.append(_tokens)\n",
        "      label.append(_label)\n",
        "\n",
        "    encoding = self.tokenizer(text=tokens, **self.kwargs)\n",
        "\n",
        "    input_ids = encoding['input_ids']\n",
        "    attention_mask = encoding['attention_mask']\n",
        "\n",
        "    # construct reference token ids\n",
        "    ref_input_ids = torch.zeros_like(input_ids) # pad_token_id == 0\n",
        "\n",
        "    ref_input_ids[:, 0] = self.tokenizer.cls_token_id\n",
        "    ref_input_ids[:, -1] = self.tokenizer.sep_token_id\n",
        "\n",
        "    return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'ref_input_ids': ref_input_ids,\n",
        "            'label': torch.tensor(label)\n",
        "          }\n",
        "\n",
        "class MyMLMCollator(DataCollatorForLanguageModeling, Collator):\n",
        "  '''\n",
        "  Notes about DataCollatorForLanguageModeling:\n",
        "\n",
        "    1. It uses tokenizer.pad() that pads the sequence. This function accepts various\n",
        "    inputs(https://github.com/huggingface/transformers/blob/250032e974359ba8df38c7e5530020050240d305/src/transformers/tokenization_utils_base.py#L3130)\n",
        "    we pass List[BatchEncoding].\n",
        "\n",
        "    2. The pad() method checks if padding is necessary, so we could have padded the\n",
        "    sequence before. It pads anyway, but for another reason. it uses pad_to_multiple_of for\n",
        "    hardware efficiency, as the inputs in the batch have multiple tokens of the number that\n",
        "    hardware supports.\n",
        "\n",
        "    3. It returns batches with 'input_ids' and 'labels' inside the data flow. After the\n",
        "    collator the batch with these 2 lists are passed into the model. The 'labels' concern\n",
        "    the MLM loss (-100 for unmasked tokens) NOT classification task.\n",
        "\n",
        "    4. The 'input_ids' processed and substituted by MASK token based on the algo(80/10/10).\n",
        "\n",
        "    5. We define a parent Class. __call__() returns\n",
        "    {\n",
        "      input_ids,\n",
        "      label (target)/ ref_input_ids,\n",
        "      labels(-100 for ignoring non masked tokens),\n",
        "      attentions_mask\n",
        "    }.\n",
        "\n",
        "    6. For MLM task, it is not necessary to index the output manually, as it is done inside the conductance\n",
        "    https://github.com/pytorch/captum/blob/2efc105b9638383911191581f2617276a1512734/captum/_utils/common.py#L515\n",
        "\n",
        "  '''\n",
        "  def __init__(self, tokenizer, mlm=True, mlm_probability=0.15, **kwargs):\n",
        "\n",
        "    # Initialize the base DataCollatorForLanguageModeling class\n",
        "    DataCollatorForLanguageModeling.__init__(self, tokenizer, mlm, mlm_probability)\n",
        "    Collator.__init__(self, tokenizer, **kwargs)\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    '''\n",
        "      1. This collator changes dynamically the batch size. If we notice the data flow\n",
        "          the only use of the batch size is when creating the data loader. In our case, we use\n",
        "          backet sampling, so we implement with custom way the batches and yield each one iteratining\n",
        "          through them.\n",
        "\n",
        "          In source code, the dataloader has a function that yields the batches, but in our case it is\n",
        "          custom. So, after yielding each batch, we can process the data as we want. (data augmentation)\n",
        "    '''\n",
        "\n",
        "    encoding_of_batches = Collator.__call__(self, batch)\n",
        "\n",
        "    batch_of_encodings = [\n",
        "        {key: val if isinstance(val, list) else val for key, val in zip(encoding_of_batches.keys(), item)}\n",
        "        for item in zip(*encoding_of_batches.values())\n",
        "    ]\n",
        "\n",
        "    '''\n",
        "      **  Sometimes, due to the probabilistic algo, there is no masked token.\n",
        "          There is an issue with the target variable then. So, we use\n",
        "          the while condition until masking will be applied.\n",
        "\n",
        "          Another solution is to mask one token arbitrarily.\n",
        "    '''\n",
        "\n",
        "    count = 0\n",
        "    while(True):\n",
        "      count += 1\n",
        "\n",
        "      if count < 10:\n",
        "        final_batch = DataCollatorForLanguageModeling.__call__(self, batch_of_encodings)\n",
        "      else:\n",
        "        print('gamiemai')\n",
        "        final_batch['input_ids'][0][2] = 103\n",
        "        final_batch['labels'][0][2] = 103 # arbitrarily index = 2\n",
        "\n",
        "      labels = final_batch['labels'][0]\n",
        "\n",
        "      # Find masked tokens (80/10/10)\n",
        "      non_100_indices = torch.where(labels != -100)[0]\n",
        "\n",
        "      num_repeats = len(non_100_indices)\n",
        "\n",
        "      if num_repeats > 0:\n",
        "        break\n",
        "\n",
        "\n",
        "    ###### The following code works ONLY  for BATCH_SIZE = 1 !!! ######\n",
        "    augmented_inputs = []\n",
        "    target = []\n",
        "\n",
        "    input_ids_og = encoding_of_batches['input_ids'][0]\n",
        "    input_ids_masked = final_batch['input_ids'][0]\n",
        "\n",
        "    # We make as much copies of the initial sentenece as the number of the masked tokens\n",
        "    for indx in non_100_indices:\n",
        "      new_input_ids = input_ids_og.detach().clone() # Here the tensors are NOT in the device\n",
        "\n",
        "      new_input_ids[indx] = input_ids_masked[indx]\n",
        "      pair = (indx.item(), input_ids_og[indx].item())\n",
        "\n",
        "      augmented_inputs.append(new_input_ids)\n",
        "      target.append(pair)\n",
        "\n",
        "    final_batch['input_ids'] = torch.stack(augmented_inputs)\n",
        "    final_batch['target'] = target\n",
        "    final_batch['attention_mask'] = final_batch['attention_mask'].repeat(num_repeats, 1)\n",
        "    final_batch['ref_input_ids'] = final_batch['ref_input_ids'].repeat(num_repeats, 1)\n",
        "\n",
        "    # final_batch['label] has not the right shape :P. it was not necessary!!\n",
        "    # print(final_batch)\n",
        "\n",
        "    return final_batch\n",
        "\n",
        "class BatchSamplerSimilarLength(Sampler):\n",
        "  '''\n",
        "    DATA FLOW:\n",
        "      Dataset -> Sampler -> Collator -> DataLoader -> Model -> Training Loop\n",
        "  '''\n",
        "  def __init__(self, dataset, batch_size, indices=None, shuffle=False):\n",
        "    self.batch_size = batch_size\n",
        "    self.shuffle = shuffle\n",
        "    # get the indices and length\n",
        "    self.indices = [(i, len(s[0])) for i, s in enumerate(dataset)]\n",
        "    # if indices are passed, then use only the ones passed (for ddp)\n",
        "    if indices is not None:\n",
        "       self.indices = torch.tensor(self.indices)[indices].tolist()\n",
        "\n",
        "  def __iter__(self):\n",
        "    if self.shuffle:\n",
        "       random.shuffle(self.indices)\n",
        "\n",
        "    pooled_indices = []\n",
        "    # create pool of indices with similar lengths\n",
        "    for i in range(0, len(self.indices), self.batch_size * 100):\n",
        "      pooled_indices.extend(sorted(self.indices[i:i + self.batch_size * 100], key=lambda x: x[1]))\n",
        "    self.pooled_indices = [x[0] for x in pooled_indices]\n",
        "\n",
        "    # yield indices for current batch\n",
        "    batches = [self.pooled_indices[i:i + self.batch_size] for i in\n",
        "               range(0, len(self.pooled_indices), self.batch_size)]\n",
        "\n",
        "    if self.shuffle:\n",
        "        random.shuffle(batches)\n",
        "    for batch in batches:\n",
        "        yield batch\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.indices) // self.batch_size\n",
        "\n",
        "def flatten_tensors(tensor_list):\n",
        "    \"\"\"\n",
        "    Flatten the last two dimensions of each tensor in the list.\n",
        "    Input tensors should have shape (12, batch, 12, tokens, tokens).\n",
        "    The output tensors will have shape (12, batch, 12, flatten_tokens).\n",
        "    \"\"\"\n",
        "    return [tensor.view(12, -1, 12, tensor.size(3) * tensor.size(4)) for tensor in tensor_list]\n",
        "\n",
        "def compute_correlations_for_batches(attentions_list, attributions_list):\n",
        "    \"\"\"\n",
        "    Computes the Spearman correlations for each batch in each tensor in the lists.\n",
        "    The input lists should contain tensors of shape (12, batch, 12, flatten_tokens).\n",
        "    The output is a tensor of shape (total_batches, 12, 12) where total_batches is the sum of all batches.\n",
        "    \"\"\"\n",
        "    # Flatten the last two dimensions of each tensor\n",
        "    attentions_list = flatten_tensors(attentions_list)\n",
        "    attributions_list = flatten_tensors(attributions_list)\n",
        "\n",
        "    # List to hold the correlation results for each batch\n",
        "    all_correlation_results = []\n",
        "\n",
        "    # Iterate over each tensor in the lists\n",
        "    for attentions, attributions in zip(attentions_list, attributions_list):\n",
        "        # Ensure the tensors have the correct shape\n",
        "        if attentions.shape[0] != 12 or attributions.shape[0] != 12:\n",
        "            raise ValueError(\"Input tensors must have the first dimension of size 12\")\n",
        "\n",
        "        # Number of batches in the current tensor\n",
        "        num_batches = attentions.shape[1]\n",
        "\n",
        "        # Compute correlations for each batch\n",
        "        for batch_idx in range(num_batches):\n",
        "            # Initialize a tensor to hold the correlation results for this batch\n",
        "            correlation_tensor = torch.zeros((12, 12))\n",
        "\n",
        "            # Extract the batch-specific tensors\n",
        "            attention_batch = attentions[:, batch_idx, :, :]\n",
        "            attribution_batch = attributions[:, batch_idx, :, :]\n",
        "\n",
        "            # Iterate over each pair of layers and heads\n",
        "            for layer_idx in range(12):\n",
        "                for head_idx in range(12):\n",
        "                    # Extract the corresponding vectors for the current pair\n",
        "                    attention_vector = attention_batch[layer_idx, head_idx, :].numpy()\n",
        "                    attribution_vector = attribution_batch[layer_idx, head_idx, :].numpy()\n",
        "\n",
        "                    # Compute the Spearman correlation between these vectors\n",
        "                    correlation, _ = scipy.stats.spearmanr(attention_vector, attribution_vector)\n",
        "\n",
        "                    # Store the correlation in the result tensor\n",
        "                    correlation_tensor[layer_idx, head_idx] = correlation\n",
        "\n",
        "            # Add the correlation results of this batch to the list\n",
        "            all_correlation_results.append(correlation_tensor)\n",
        "\n",
        "    # Stack all correlation results and compute the mean across the batch dimension\n",
        "    stacked_correlations = torch.stack(all_correlation_results)\n",
        "    mean_correlation_tensor = stacked_correlations.mean(dim=0)\n",
        "\n",
        "\n",
        "    ### Same process - to be refactored ###\n",
        "    SpearMetric = mean_correlation_tensor.clone().detach()\n",
        "\n",
        "    # Note the sign before parse absolute value\n",
        "    SpearSign = torch.ones_like(SpearMetric)\n",
        "\n",
        "    SpearSign[torch.where(SpearMetric < 0)] = -1\n",
        "    SpearSign[torch.where(SpearMetric == 0)] = 0\n",
        "\n",
        "    SpearMetric = torch.abs(SpearMetric)\n",
        "\n",
        "    return SpearMetric.flatten(), SpearSign.flatten()\n"
      ],
      "metadata": {
        "id": "c-b89P5WfLwD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fOu0upkVbVgB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### main"
      ],
      "metadata": {
        "id": "Iug2kCwT0aLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AutoModelForMaskedLM\n",
        "\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "\n",
        "import time\n",
        "\n",
        "def main(path, model_path, dataset_name, D_size, RANDOM_SEED, BATCH_SIZE, target, MAX_LEN, mlm=False):\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # load model\n",
        "    if mlm:\n",
        "      model = AutoModelForMaskedLM.from_pretrained(model_path, output_attentions=True)\n",
        "    else:\n",
        "      model = BertForSequenceClassification.from_pretrained(model_path, output_attentions=True)\n",
        "    # eval mode and deactivate gradients to avoid memory leak\n",
        "    model.eval()\n",
        "    model.zero_grad()\n",
        "\n",
        "    # transfer the model to device\n",
        "    model.to(device)\n",
        "\n",
        "    # load tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "\n",
        "    dataset = load_dataset(dataset_name)\n",
        "\n",
        "    # Take the whole dataset, for being more valid\n",
        "    datasets_to_concatenate = [ds for name, ds in dataset.items() if name != 'unsupervised']\n",
        "\n",
        "    # Concatenate the remaining datasets\n",
        "    dataset = concatenate_datasets(datasets_to_concatenate)\n",
        "\n",
        "    # Normalize the columns\n",
        "    dataset = normalize_columns(dataset, 'sentence', 'text')\n",
        "\n",
        "    # Tokenize and filter dataset\n",
        "    dataset = tokenize_and_filter(dataset, tokenizer, max_token_length=MAX_LEN)\n",
        "\n",
        "    # define parameters for stratified splitting\n",
        "    D_tokens = np.array(dataset[\"tokens\"], dtype='object')\n",
        "    D_label = np.array(dataset[\"label\"])\n",
        "\n",
        "    # split the dataset with stratification\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=D_size, random_state=RANDOM_SEED)\n",
        "    _, test_index = list(sss.split(D_tokens, D_label))[0]\n",
        "\n",
        "    x_test, y_test = D_tokens[test_index], D_label[test_index]\n",
        "\n",
        "    stratified_data = {\n",
        "        \"tokens\": x_test,\n",
        "        \"label\": y_test\n",
        "    }\n",
        "\n",
        "    # pass the stratified dataset\n",
        "    data = pd.DataFrame(data=stratified_data)\n",
        "\n",
        "    del dataset, D_label, D_tokens\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    # Define parameters for Collator(tokenizer)\n",
        "    kwargs = {\n",
        "        \"add_special_tokens\": True,\n",
        "        \"return_token_type_ids\": False,\n",
        "        \"max_length\": MAX_LEN + 2, # 1 for cls, 1 for sep\n",
        "        \"padding\": True,\n",
        "        \"return_attention_mask\": True,\n",
        "        \"truncation\": True, # it is already done in pre-processing\n",
        "        \"return_tensors\": \"pt\",\n",
        "        \"is_split_into_words\": True\n",
        "    }\n",
        "\n",
        "    # Create data loader\n",
        "    if mlm:\n",
        "      _collator = MyMLMCollator(tokenizer=tokenizer, mlm=mlm, mlm_probability=0.15, **kwargs)\n",
        "      data_loader = create_data_loader(data, BATCH_SIZE, tokenizer, _collator, bucket_sampling=True, shuffle=False)\n",
        "\n",
        "      total_attentions, total_attributions = get_matrices(\n",
        "          model,\n",
        "          device,\n",
        "          data_loader\n",
        "      )\n",
        "\n",
        "      torch.save(total_attentions, path + 'attentions.pt')\n",
        "      torch.save(total_attributions, path + 'attributions.pt')\n",
        "\n",
        "      del total_attentions, total_attributions\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    else:\n",
        "      _collator = Collator(tokenizer=tokenizer, **kwargs)\n",
        "      data_loader = create_data_loader(data, BATCH_SIZE, tokenizer, _collator, bucket_sampling=True, shuffle=False)\n",
        "\n",
        "      # Attentions Computation\n",
        "\n",
        "      total_attentions = get_predictions(\n",
        "          model,\n",
        "          device,\n",
        "          data_loader\n",
        "      )\n",
        "\n",
        "      torch.save(total_attentions, path + 'attentions.pt')\n",
        "      del total_attentions\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      # Attributions Computation\n",
        "\n",
        "      total_attributions = get_interpretability_scores(\n",
        "          model,\n",
        "          device,\n",
        "          data_loader,\n",
        "          target=target\n",
        "      )\n",
        "\n",
        "      torch.save(total_attributions, path + 'attributions.pt')\n",
        "      del total_attributions\n",
        "      torch.cuda.empty_cache()\n",
        "\n"
      ],
      "metadata": {
        "id": "kYryeejimQoG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### stats"
      ],
      "metadata": {
        "id": "ptPVwLxn72TA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def corr_computation(matrixA, matrixB):\n",
        "\n",
        "    # Compute correlation matrix\n",
        "    shape = matrixA.shape\n",
        "\n",
        "    # Reshape tensors to 2D\n",
        "    matrixA_reshaped = matrixA.reshape(-1, shape[-1])\n",
        "    matrixB_reshaped = matrixB.reshape(-1, shape[-1])\n",
        "\n",
        "    # Compute Spearman correlation for each row\n",
        "    correlation, _ = scipy.stats.spearmanr(matrixA_reshaped.detach().clone().cpu(), matrixB_reshaped.detach().clone().cpu(), axis=1)\n",
        "\n",
        "    # Extract diagonal elements to get the correlation between matrixA[i][j] and matrixB[i][j]\n",
        "    n = matrixB_reshaped.shape[0]\n",
        "    correlation_diagonal = np.diag(correlation[n:, :n])  # 12*12 = 144\n",
        "\n",
        "    # Reshape correlation values back to (12, 12)\n",
        "    correlation_matrix = correlation_diagonal.reshape(shape[:-1])\n",
        "\n",
        "    ### Same process - to be refactored ###\n",
        "    SpearMetric = torch.tensor(correlation_matrix)\n",
        "\n",
        "    # Note the sign before parse absolute value\n",
        "    SpearSign = torch.ones_like(SpearMetric)\n",
        "\n",
        "    SpearSign[torch.where(SpearMetric < 0)] = -1\n",
        "    SpearSign[torch.where(SpearMetric == 0)] = 0\n",
        "\n",
        "    SpearMetric = torch.abs(SpearMetric)\n",
        "\n",
        "    return SpearMetric.flatten(), SpearSign.flatten()\n",
        "\n",
        "\n",
        "def corr_computation_outlier_rem(matrixA, matrixB):\n",
        "\n",
        "    correlation_matrix = []\n",
        "    for i in range(12):\n",
        "        for j in range(12):\n",
        "\n",
        "            data_stacked = pd.DataFrame(torch.stack((matrixA[i][j].cpu(), matrixB[i][j].cpu()), 0).numpy().T, columns = [f'L{i+1}-H{j+1}-A', f'L{i+1}-H{j+1}-Attr'])\n",
        "\n",
        "            # find absolute value of z-score for each observation\n",
        "            z = np.abs(scipy.stats.zscore(data_stacked))\n",
        "\n",
        "            # keep only rows in dataframe with all z-scores less than absolute value of 3\n",
        "            data_clean = data_stacked[(z < 3).all(axis=1)]\n",
        "\n",
        "            score = data_clean[f'L{i+1}-H{j+1}-A'].corr(data_clean[f'L{i+1}-H{j+1}-Attr'], method='spearman')\n",
        "            correlation_matrix.append(score)\n",
        "\n",
        "    ### Same process - to be refactored ###\n",
        "    SpearMetricOutRemoval = torch.tensor(correlation_matrix)\n",
        "\n",
        "    SpearSignOutRemoval = torch.ones_like(SpearMetricOutRemoval)\n",
        "\n",
        "    SpearSignOutRemoval[torch.where(SpearMetricOutRemoval < 0)] = -1\n",
        "    SpearSignOutRemoval[torch.where(SpearMetricOutRemoval == 0)] = 0\n",
        "\n",
        "    SpearMetricOutRemoval = torch.abs(SpearMetricOutRemoval)\n",
        "\n",
        "    return SpearMetricOutRemoval, SpearSignOutRemoval\n",
        "\n",
        "\n",
        "def main2(path):\n",
        "\n",
        "    attentions_list = torch.load(path + 'attentions.pt')\n",
        "    attributions_list = torch.load(path + 'attributions.pt')\n",
        "\n",
        "    # Implementation of the second idea\n",
        "    # I_{L,H} = E_x(corr(Attr(A_{L,H}), A_{L,H})\n",
        "\n",
        "    SpearMetric, SpearSign = compute_correlations_for_batches(attentions_list, attributions_list)\n",
        "\n",
        "    params = [(0.2, 0.4), (0.4, 0.6), (0.6, 0.8), (0.8, 1.0)]\n",
        "    condition = lambda n, p: (n >= p[0]) & (n < p[1])\n",
        "\n",
        "    result, indices = calculate_percentages(SpearMetric, condition, params)\n",
        "\n",
        "    # Find which heads are strongly correlated\n",
        "    visualization = torch.zeros((12, 12)).detach()\n",
        "\n",
        "    for item in indices.items():\n",
        "        for i in item[1]:\n",
        "            visualization[i // 12][i % 12] = f(item[0])\n",
        "\n",
        "    torch.save(visualization, path + 'visualization.pt')\n",
        "\n",
        "    params = [1, -1]\n",
        "    condition = lambda n, p: (n == p)\n",
        "\n",
        "    # Filename for storine the statistics\n",
        "    filename = path + 'stats.txt'\n",
        "\n",
        "    with open(filename, 'w') as file:\n",
        "        for (r, p), ind in zip(result.items(), indices.items()):\n",
        "            file.write(f\"Percentage of correlation scores between {r[0]} and {r[1]}: {p:.2f}%\\n\")\n",
        "\n",
        "            ind_result, _ = calculate_percentages(SpearSign[ind[1]], condition, params)\n",
        "\n",
        "            for ind in ind_result.items():\n",
        "                file.write(f\"\\tFrom which the percentage of {'positive' if ind[0] == 1 else 'negative'} scores: {ind[1]:.2f}%\\n\")\n",
        "\n",
        "\n",
        "    # # Compute the correlation - Outliers Removal\n",
        "    # SpearMetricOutRemoval, SpearSignOutRemoval = corr_computation_outlier_rem(attentions_mean, attributions_mean)\n",
        "\n",
        "\n",
        "    # params = [(0.2, 0.4), (0.4, 0.6), (0.6, 0.8), (0.8, 1.0)]\n",
        "    # condition = lambda n, p: (n >= p[0]) & (n < p[1])\n",
        "\n",
        "    # result, indices = calculate_percentages(SpearMetricOutRemoval, condition, params)\n",
        "\n",
        "    # # Find which heads are strongly correlated\n",
        "    # visualization = torch.zeros((12, 12)).detach()\n",
        "\n",
        "    # for item in indices.items():\n",
        "    #     for i in item[1]:\n",
        "    #         visualization[i // 12][i % 12] = f(item[0])\n",
        "\n",
        "    # torch.save(visualization, path + '-visualization_outrem.pt')\n",
        "\n",
        "    # params = [1, -1]\n",
        "    # condition = lambda n, p: (n == p)\n",
        "\n",
        "    # # Filename for storine the statistics without outliers\n",
        "    # filename = path + '-stats_outremoval.txt'\n",
        "\n",
        "    # with open(filename, 'w') as file:\n",
        "    #     for (r, p), ind in zip(result.items(), indices.items()):\n",
        "    #         file.write(f\"Percentage of correlation scores between {r[0]} and {r[1]}: {p:.2f}%\\n\")\n",
        "\n",
        "    #         ind_result, _ = calculate_percentages(SpearSignOutRemoval[ind[1]], condition, params)\n",
        "\n",
        "    #         for ind in ind_result.items():\n",
        "    #             file.write(f\"\\tFrom which the percentage of {'positive' if ind[0] == 1 else 'negative'} scores: {ind[1]:.2f}%\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Pc7z1Pmj71gI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### run"
      ],
      "metadata": {
        "id": "FReD4u3m0d7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import subprocess\n",
        "\n",
        "\n",
        "# Initialize global configuration\n",
        "D_size = 10\n",
        "RANDOM_SEED = 0\n",
        "BATCH_SIZE = 1\n",
        "MAX_LEN = 200\n",
        "\n",
        "targets = [None, 0, 1]\n",
        "\n",
        "# Config for fine-tuned models\n",
        "\n",
        "# models_datasets = [\n",
        "#     ('textattack/bert-base-uncased-imdb', 'imdb'),\n",
        "#     ('textattack/bert-base-uncased-SST-2', 'sst2'),\n",
        "#     ('textattack/bert-base-uncased-rotten-tomatoes', 'rotten_tomatoes')\n",
        "# ]\n",
        "# model_infos = ['fine_tuned']\n",
        "\n",
        "# # Get the cartesian product of the parameters\n",
        "# combos = list(itertools.product(models_datasets, targets, model_infos))\n",
        "\n",
        "\n",
        "\n",
        "# subprocess.run([\n",
        "#         'python',\n",
        "#         'scores_computation.py',\n",
        "#         '--model', str('textattack/bert-base-uncased-imdb'),\n",
        "#         '--dataset', str('sst2'),\n",
        "#         '--target', str(None),\n",
        "#         '--tokens', str(MAX_LEN),\n",
        "#         '--model_info', str('fine-tuned'),\n",
        "#         '--D_size', str(D_size),\n",
        "#         '--seed', str(RANDOM_SEED),\n",
        "#         '--batch', str(BATCH_SIZE),\n",
        "#         ])\n",
        "\n",
        "# for ((model, dataset), target, model_info) in tqdm(combos, desc=\"Iterate through the variant configurations\", ncols=75, leave=True, bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt}\"):\n",
        "\n",
        "#     subprocess.run([\n",
        "#         'python',\n",
        "#         'scores_computation.py',\n",
        "#         '--model', str(model),\n",
        "#         '--dataset', str(dataset),\n",
        "#         '--target', str(target),\n",
        "#         '--tokens', str(MAX_LEN),\n",
        "#         '--model_info', str(model_info),\n",
        "#         '--D_size', str(D_size),\n",
        "#         '--seed', str(RANDOM_SEED),\n",
        "#         '--batch', str(BATCH_SIZE),\n",
        "#         ])\n",
        "\n",
        "\n",
        "# Begin stats extraction\n",
        "\n",
        "# for ((_, dataset), target, model_info) in tqdm(combos, desc=\"Iterate through the variant configurations for stats extraction\", ncols=75, leave=True, bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt}\"):\n",
        "\n",
        "#     subprocess.run([\n",
        "#         'python',\n",
        "#         'stats_extraction.py',\n",
        "#         '--dataset', str(dataset),\n",
        "#         '--target', str(target),\n",
        "#         '--tokens', str(MAX_LEN),\n",
        "#         '--model_info', str(model_info),\n",
        "#         ])\n",
        "\n",
        "'''\n",
        "TODO:\n",
        " 1. I should pass also 'mlm' argument as input for main.py for controlling the collator.\n",
        "'''\n",
        "# Config for pre-trained bert-base-uncased\n",
        "\n",
        "model = ['bert-base-uncased']\n",
        "# model = ['textattack/bert-base-uncased-rotten-tomatoes']\n",
        "\n",
        "# model = ['pbelcak/UltraFastBERT-1x11-long']\n",
        "datasets = [ 'rotten_tomatoes', 'sst2', 'imdb' ]\n",
        "model_infos = ['pre_trained']\n",
        "\n",
        "# Get the cartesian product of the parameters\n",
        "combos = list(itertools.product(model, datasets, model_infos))\n",
        "\n",
        "MLM = True\n",
        "config = {}\n",
        "\n",
        "for (model, dataset, model_info) in tqdm(combos, desc=\"Iterate through the variant configurations\", ncols=75, leave=True, bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt}\"):\n",
        "\n",
        "\n",
        "  # update config\n",
        "  config['dataset'] = dataset\n",
        "  config['target'] = 'class_reduce'\n",
        "  config['max_len'] = f'{MAX_LEN}tokens'\n",
        "  config['model_info'] = model_info\n",
        "\n",
        "  # path construction\n",
        "  path = create_nested_directories(config)\n",
        "\n",
        "  kwargs = {\n",
        "      'path': path,\n",
        "      'model_path': model,\n",
        "      'dataset_name': dataset,\n",
        "      'D_size': D_size,\n",
        "      'RANDOM_SEED': RANDOM_SEED,\n",
        "      'BATCH_SIZE': BATCH_SIZE,\n",
        "      'target': 0,#None if target == 'None' else target,\n",
        "      'MAX_LEN':  MAX_LEN,\n",
        "      'mlm': MLM\n",
        "  }\n",
        "\n",
        "  # define variable for time measurement\n",
        "  start_time = time.time()\n",
        "\n",
        "  # call the main() function for scores computation and storing\n",
        "  main(**kwargs)\n",
        "\n",
        "  print(f\"{path} --- {time.time() - start_time} seconds ---\")\n",
        "  # break\n",
        "\n",
        "# Begin stats extraction\n",
        "\n",
        "# for (_, dataset, target, model_info) in tqdm(combos, desc=\"Iterate through the variant configurations for stats extraction\", ncols=75, leave=True, bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt}\"):\n",
        "\n",
        "#     subprocess.run([\n",
        "#         'python',\n",
        "#         'stats_extraction.py',\n",
        "#         '--dataset', str(dataset),\n",
        "#         '--target', str(target),\n",
        "#         '--tokens', str(MAX_LEN),\n",
        "#         '--model_info', str(model_info),\n",
        "#         ])\n"
      ],
      "metadata": {
        "id": "u-JQL6jyrDX4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bcee52eb68d84194a5dd097ec5f1d58c",
            "0d5896c10e9045108aeff0411706cb12",
            "5ed0cc4993194163bfd81da9af73f75f",
            "872300c8e11b4803a01f0c44cd31daff",
            "6605f0e8ef4d445c95b211f227ca7609",
            "3fab5bd4fb8b4885bac3084d38a7cb7b",
            "6265bcb7490440789482a45393822088",
            "32132035bb8c4c11a0c8604827937656",
            "96abe103d8d146f2b9751ed2409aa6ef",
            "dab7e1670a824d7ca146695292389e6d",
            "c4c4fd63857b4a688668d6238be1a591",
            "7bd4f1b86ef24ba7bcbca481faf20f8a",
            "b3fa4c37a0824494a952ae61e06c88e5",
            "7a91db6b0bfe4cf6a600dfa3f50a4a0f",
            "b822963a9aae497d8b5267a73c508e9f",
            "c723d56882eb4cecac7c8c32226602f2",
            "7a5e7ec19a094621aba637bc25113521",
            "d3222db647ff4a019545d0268d017d53",
            "3f4c53d10c534ddeb3210587f69eaf3c",
            "bb28b81c144040bcaeebaba7a385eb82",
            "d08d261e48994e8e87acc7e7cf12033e",
            "ccca812d8cac413fa3623aa527249908"
          ]
        },
        "outputId": "4a67b73e-bdfc-485d-d2cf-4707f60a9c3a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rIterate through the variant configurations:   0%|                     | 0/3Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The column 'sentence' does not exist in the dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Calculating Both Matrices. It may take a while :P...:   0%|          | 0/10\u001b[A/usr/local/lib/python3.10/dist-packages/captum/attr/_models/base.py:191: UserWarning: In order to make embedding layers more interpretable they will be replaced with an interpretable embedding layer which wraps the original embedding layer and takes word embedding vectors as inputs of the forward function. This allows us to generate baselines for word embeddings and compute attributions for each embedding dimension. The original embedding layer must be set back by calling `remove_interpretable_embedding_layer` function after model interpretation is finished. \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 18])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Calculating Both Matrices. It may take a while :P...:  10%|█         | 1/10\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([7, 27])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Calculating Both Matrices. It may take a while :P...:  20%|██        | 2/10\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 24])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Calculating Both Matrices. It may take a while :P...:  30%|███       | 3/10\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 27])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Calculating Both Matrices. It may take a while :P...:  40%|████      | 4/10\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 33])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Calculating Both Matrices. It may take a while :P...:  50%|█████     | 5/10\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([7, 42])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Calculating Both Matrices. It may take a while :P...:  60%|██████    | 6/10\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 41])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Calculating Both Matrices. It may take a while :P...:  70%|███████   | 7/10\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 37])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Calculating Both Matrices. It may take a while :P...:  80%|████████  | 8/10\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 40])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Calculating Both Matrices. It may take a while :P...:  90%|█████████ | 9/10\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([9, 49])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Calculating Both Matrices. It may take a while :P...: 100%|█████████| 10/10\n",
            "Iterate through the variant configurations:  33%|███████              | 1/3"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "../scores/rotten_tomatoes/class_reduce/200tokens/pre_trained/ --- 173.55858159065247 seconds ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\n",
            "Calculating Both Matrices. It may take a while :P...:   0%|          | 0/10\u001b[A/usr/local/lib/python3.10/dist-packages/captum/attr/_models/base.py:191: UserWarning: In order to make embedding layers more interpretable they will be replaced with an interpretable embedding layer which wraps the original embedding layer and takes word embedding vectors as inputs of the forward function. This allows us to generate baselines for word embeddings and compute attributions for each embedding dimension. The original embedding layer must be set back by calling `remove_interpretable_embedding_layer` function after model interpretation is finished. \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Calculating Both Matrices. It may take a while :P...:  10%|█         | 1/10\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 8])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Calculating Both Matrices. It may take a while :P...:  20%|██        | 2/10\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 17])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Calculating Both Matrices. It may take a while :P...:  30%|███       | 3/10\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 34])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Calculating Both Matrices. It may take a while :P...:  40%|████      | 4/10\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([7, 36])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Calculating Both Matrices. It may take a while :P...:  50%|█████     | 5/10\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([7, 41])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Calculating Both Matrices. It may take a while :P...:  60%|██████    | 6/10\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 34])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Calculating Both Matrices. It may take a while :P...:  70%|███████   | 7/10\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 50])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Calculating Both Matrices. It may take a while :P...:  80%|████████  | 8/10\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([7, 49])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Calculating Both Matrices. It may take a while :P...:  90%|█████████ | 9/10\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 66])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Calculating Both Matrices. It may take a while :P...: 100%|█████████| 10/10\n",
            "Iterate through the variant configurations:  67%|██████████████       | 2/3"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "../scores/sst2/class_reduce/200tokens/pre_trained/ --- 186.11310744285583 seconds ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The column 'sentence' does not exist in the dataset.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bcee52eb68d84194a5dd097ec5f1d58c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bd4f1b86ef24ba7bcbca481faf20f8a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Calculating Both Matrices. It may take a while :P...:   0%|          | 0/10\u001b[A/usr/local/lib/python3.10/dist-packages/captum/attr/_models/base.py:191: UserWarning: In order to make embedding layers more interpretable they will be replaced with an interpretable embedding layer which wraps the original embedding layer and takes word embedding vectors as inputs of the forward function. This allows us to generate baselines for word embeddings and compute attributions for each embedding dimension. The original embedding layer must be set back by calling `remove_interpretable_embedding_layer` function after model interpretation is finished. \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([25, 129])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating Both Matrices. It may take a while :P...:   0%|          | 0/10\n",
            "Iterate through the variant configurations:  67%|██████████████       | 2/3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-4b7dd06882a0>\u001b[0m in \u001b[0;36m<cell line: 89>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m   \u001b[0;31m# call the main() function for scores computation and storing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{path} --- {time.time() - start_time} seconds ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-4ada0faa4cc9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(path, model_path, dataset_name, D_size, RANDOM_SEED, BATCH_SIZE, target, MAX_LEN, mlm)\u001b[0m\n\u001b[1;32m     85\u001b[0m       \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_collator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket_sampling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m       total_attentions, total_attributions = get_matrices(\n\u001b[0m\u001b[1;32m     88\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m           \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-dbf5e5afeca0>\u001b[0m in \u001b[0;36mget_matrices\u001b[0;34m(model, device, data_loader)\u001b[0m\n\u001b[1;32m    168\u001b[0m           \u001b[0;31m# Delete intermediate tensors to free up memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m           \u001b[0;32mdel\u001b[0m \u001b[0mlayer_attributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m           \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# (12,1,12,tokens,tokens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py\u001b[0m in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m     \"\"\"\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_emptyCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main2('../scores/sst2/class_reduce/200tokens/pre_trained/')"
      ],
      "metadata": {
        "id": "OddmYnwj8OdK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# load the data\n",
        "visualization = torch.load('../scores/rotten_tomatoes/class_reduce/200tokens/pre_trained/' + 'visualization.pt')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,5))\n",
        "xticklabels=list(range(1,13))\n",
        "yticklabels=list(range(1,13))\n",
        "ax = sns.heatmap(visualization.cpu().detach().numpy(), xticklabels=xticklabels, yticklabels=yticklabels, linewidth=0.2)\n",
        "plt.xlabel('Heads')\n",
        "plt.ylabel('Layers')\n",
        "plt.title(config['dataset'] + '-' + config['model_info'] + '-' + config['target'] + '-'+ config['max_len'])\n",
        "# plt.savefig(path + 'visualization_fig')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VS3Ko9Q98dfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "outputId": "1192a2a0-68f5-4d1e-be22-626a9fd4007f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ../scores/rotten_tomatoes/NoneClass/50tokens/pre_trained/"
      ],
      "metadata": {
        "id": "3VY5SFgt507d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8aaefdb-db7b-43d4-c267-7305de1f6ffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attributions.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nano ../scores/rotten_tomatoes/NoneClass/50tokens/pre_trained/stats.txt"
      ],
      "metadata": {
        "id": "d4mdnY9Z8tvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attributions_list = torch.load(path + 'attributions.pt')\n",
        "print(len(attributions_list))\n",
        "print(attributions_list[0].shape)"
      ],
      "metadata": {
        "id": "Wk-RSzYt6BvW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb921e1c-930a-40d8-a2b9-3223c28df934"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "torch.Size([12, 12, 18, 18])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attentions_list = torch.load(path + 'attentions.pt')\n",
        "print(len(attentions_list))\n",
        "print(attentions_list[0].shape)"
      ],
      "metadata": {
        "id": "BMbumU4K7SJT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be17115a-024e-4e3a-9110-08d9328c7bb3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "torch.Size([12, 12, 18, 18])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "a = torch.tensor([1, 2, 3, 4, 5, 6])\n",
        "num_repeats = 3\n",
        "\n",
        "b = a\n",
        "# b = a.repeat(num_repeats).view(num_repeats, -1)\n",
        "\n",
        "print(b)\n"
      ],
      "metadata": {
        "id": "oxMJSOAB2j_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "a = torch.tensor([1, 2, 3, 4, 5, 6])\n",
        "num_copies = 3\n",
        "\n",
        "# Using repeat\n",
        "# b = a.unsqueeze(0).repeat(num_copies, 1)\n",
        "b = a.repeat(num_repeats, 1).clone()\n",
        "\n",
        "# b = a.unsqueeze(0).repeat(num_copies, 1).clone()\n",
        "\n",
        "# b = a.unsqueeze(0).expand(num_copies, -1)\n",
        "# Modifying the first element of b\n",
        "b[0, 1] = 0\n",
        "b[0, 3] = 0\n",
        "b[0, 4] = 0\n",
        "print(b)\n",
        "print(a)  # This will print tensor([0, 2, 3, 4, 5, 6])\n"
      ],
      "metadata": {
        "id": "zTFThzBOyCOU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MVYfvZ3p0kXf",
        "Iug2kCwT0aLg",
        "ptPVwLxn72TA"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bcee52eb68d84194a5dd097ec5f1d58c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d5896c10e9045108aeff0411706cb12",
              "IPY_MODEL_5ed0cc4993194163bfd81da9af73f75f",
              "IPY_MODEL_872300c8e11b4803a01f0c44cd31daff"
            ],
            "layout": "IPY_MODEL_6605f0e8ef4d445c95b211f227ca7609"
          }
        },
        "0d5896c10e9045108aeff0411706cb12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fab5bd4fb8b4885bac3084d38a7cb7b",
            "placeholder": "​",
            "style": "IPY_MODEL_6265bcb7490440789482a45393822088",
            "value": "Map: 100%"
          }
        },
        "5ed0cc4993194163bfd81da9af73f75f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32132035bb8c4c11a0c8604827937656",
            "max": 50000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96abe103d8d146f2b9751ed2409aa6ef",
            "value": 50000
          }
        },
        "872300c8e11b4803a01f0c44cd31daff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dab7e1670a824d7ca146695292389e6d",
            "placeholder": "​",
            "style": "IPY_MODEL_c4c4fd63857b4a688668d6238be1a591",
            "value": " 50000/50000 [04:27&lt;00:00, 196.67 examples/s]"
          }
        },
        "6605f0e8ef4d445c95b211f227ca7609": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fab5bd4fb8b4885bac3084d38a7cb7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6265bcb7490440789482a45393822088": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32132035bb8c4c11a0c8604827937656": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96abe103d8d146f2b9751ed2409aa6ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dab7e1670a824d7ca146695292389e6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4c4fd63857b4a688668d6238be1a591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bd4f1b86ef24ba7bcbca481faf20f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3fa4c37a0824494a952ae61e06c88e5",
              "IPY_MODEL_7a91db6b0bfe4cf6a600dfa3f50a4a0f",
              "IPY_MODEL_b822963a9aae497d8b5267a73c508e9f"
            ],
            "layout": "IPY_MODEL_c723d56882eb4cecac7c8c32226602f2"
          }
        },
        "b3fa4c37a0824494a952ae61e06c88e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a5e7ec19a094621aba637bc25113521",
            "placeholder": "​",
            "style": "IPY_MODEL_d3222db647ff4a019545d0268d017d53",
            "value": "Filter: 100%"
          }
        },
        "7a91db6b0bfe4cf6a600dfa3f50a4a0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f4c53d10c534ddeb3210587f69eaf3c",
            "max": 50000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bb28b81c144040bcaeebaba7a385eb82",
            "value": 50000
          }
        },
        "b822963a9aae497d8b5267a73c508e9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d08d261e48994e8e87acc7e7cf12033e",
            "placeholder": "​",
            "style": "IPY_MODEL_ccca812d8cac413fa3623aa527249908",
            "value": " 50000/50000 [00:15&lt;00:00, 3887.45 examples/s]"
          }
        },
        "c723d56882eb4cecac7c8c32226602f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a5e7ec19a094621aba637bc25113521": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3222db647ff4a019545d0268d017d53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f4c53d10c534ddeb3210587f69eaf3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb28b81c144040bcaeebaba7a385eb82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d08d261e48994e8e87acc7e7cf12033e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccca812d8cac413fa3623aa527249908": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}